{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import random\n",
    "import math\n",
    "import sklearn.linear_model\n",
    "import sys\n",
    "import torch.autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "\n",
    "# Import relevant files\n",
    "filename1 = 'train_set_attr_scld.csv'\n",
    "train_set_attr_scld = genfromtxt(filename2, delimiter=',')\n",
    "\n",
    "filename2 = 'test_set_scld.csv'\n",
    "test_set_scld = genfromtxt(filename3, delimiter=',')\n",
    "\n",
    "# ExperimentsCNN.txt contains parameters for the neural network as follows:\n",
    "# experiment num, iter, lrate, ovrsmpl, epochnum, ftr_size, ftr_1, ftr_2 ... ftr_n\n",
    "\n",
    "filename = 'experimentsCNN.txt'\n",
    "lineNum = 48\n",
    "countIter = -1\n",
    "# Read input file to run processes in parallel on the cluster\n",
    "with open(filename) as f:\n",
    "    for line in f:\n",
    "        lineNum = lineNum + 1\n",
    "        if(lineNum == int(sys.argv[1])):\n",
    "            entries = line.split(\",\")\n",
    "            iterations = int(entries[1])\n",
    "            lrate = float(entries[2])\n",
    "            ovrsmpl = int(entries[3])\n",
    "            epochNum = int(entries[4])\n",
    "            ftr_size = int(entries[5])\n",
    "            ftrs = list()\n",
    "            for feature in range(ftr_size):\n",
    "                ftrs.append(int(entries[6 + feature]))\n",
    "    \n",
    "output = np.empty((10,3))\n",
    "\n",
    "# Loop through every iteration/trial \n",
    "for iteration in (range(iterations)):\n",
    "    feature_size = ftr_size\n",
    "    \n",
    "#   Define network architecture\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()  \n",
    "            self.conv1 = nn.Conv1d(ftr_size, 45, 1) \n",
    "            self.conv2 = nn.Conv1d(45, 15, 1)\n",
    "            self.fc1 = nn.Linear(15, 120) \n",
    "            self.fc2 = nn.Linear(120, 84)\n",
    "            self.fc3 = nn.Linear(84, 3)     \n",
    "\n",
    "        def forward(self, x):\n",
    "            x =x.unsqueeze(dim = 2)\n",
    "            x = self.conv1(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.max_pool1d(x, 1)\n",
    "            x = self.conv2(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.max_pool1d(x, 1)\n",
    "            x = x.view(-1, self.num_flat_features(x))\n",
    "            x = self.fc1(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.fc2(x)\n",
    "            x = F.relu(x)\n",
    "            x = self.fc3(x)  \n",
    "            x = F.log_softmax(x, dim = 1)\n",
    "            return x\n",
    "        \n",
    "        def num_flat_features(self, x):\n",
    "            size = x.size()[1:]\n",
    "            num_features = 1\n",
    "            for s in size:\n",
    "                num_features *= s\n",
    "            return num_features\n",
    "\n",
    "#   Create instance of network architecture\n",
    "    net = Net()\n",
    "    \n",
    "#  Set up optimizer (stochastic gradient descent) and loss criterion (cross entropy)\n",
    "    learning_rate = lrate\n",
    "    p = 0.9\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=p)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    #  Creating the training and testing set loaders\n",
    "    import torch.utils.data \n",
    "    batch_size = 100\n",
    "\n",
    "#   Dataset subclass: EphysDataset that returns each row of the array as a PyTorch Tensor\n",
    "    class EphysDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, numpy_arr):\n",
    "            self.numpy_arr = numpy_arr\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.numpy_arr)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            arr = self.numpy_arr[index,:]\n",
    "            sample = torch.from_numpy(arr)\n",
    "            sample = sample.type(torch.FloatTensor)\n",
    "            return arr\n",
    "\n",
    "#   Convert NumPy array to PyTorch Dataset\n",
    "    train_set_attr_dtst = EphysDataset(train_set_attr_scld)\n",
    "    test_set_dtst = EphysDataset(test_set_scld)\n",
    "\n",
    "#  Load Datasets with DataLoader        \n",
    "    train_loader_attr = torch.utils.data.DataLoader(train_set_attr_dtst, batch_size = batch_size, shuffle = True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set_dtst, batch_size = batch_size, shuffle = True)\n",
    "    \n",
    "    inputDataset = train_loader_attr\n",
    "        \n",
    "    # Sample 2D array (eg. for one cell type)\n",
    "    # [unit1: [fA nA pA wTP wPT isi reg b wvfrm(29)]\n",
    "    #  unit2: [fA nA pA wTP wPT isi reg b wvfrm(29)]\n",
    "    # ..........................\n",
    "    #  unitN: [fA nA pA wTP wPT isi reg b wvfrm(29)]]\n",
    "    \n",
    "#   Training the network\n",
    "    epochs = epochNum\n",
    "    log_interval= 1000\n",
    "    \n",
    "    for epoch in range(epochs): \n",
    "        \n",
    "        for batch_idx, arr in enumerate(inputDataset):\n",
    "            \n",
    "#           Extract the data and target separately and make them PyTorch Tensors\n",
    "            data = arr[:,ftrs]\n",
    "            target = arr[:,0] - 1\n",
    "            data = data.type(torch.FloatTensor)\n",
    "            target = target.type(torch.LongTensor)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "#           Pass data to NN and calculate loss\n",
    "            net_out = net(data)\n",
    "            loss = criterion(net_out, target)\n",
    "        \n",
    "#           Backpropogation and gradient descent\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "#   Testing the network\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for arr in test_loader:\n",
    "#       Create data and target tensors\n",
    "        idx = -1;\n",
    "        data = arr[:,ftrs]\n",
    "        target = arr[:,0] - 1\n",
    "        target = target.numpy() \n",
    "        for num in target:\n",
    "            idx = idx + 1\n",
    "            if num == -1:\n",
    "                target[idx] = None\n",
    "        target = torch.from_numpy(target)\n",
    "        target = target.type(torch.LongTensor)\n",
    "        data = data.view(-1, feature_size)\n",
    "        data = data.type(torch.FloatTensor)\n",
    "        data, target = torch.autograd.Variable(data), torch.autograd.Variable(target)\n",
    "#       Test the network on validation data\n",
    "        net_out = net(data)  \n",
    "    \n",
    "#       Sum up batch loss\n",
    "        loss = criterion(net_out, target)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "#       Get the index of the max log-probability\n",
    "        pred = net_out.data.max(1)[1]  \n",
    "        correct += pred.eq(target.data).sum()\n",
    "\n",
    "    countIter += 1\n",
    "    output[countIter, 0] = test_loss\n",
    "    output[countIter, 1] = correct\n",
    "    output[countIter, 2] = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "np.savetxt('output' + str(sys.argv[1]) + '.csv', output, delimiter = \",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
