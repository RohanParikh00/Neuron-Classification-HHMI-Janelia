{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import random\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import sklearn.linear_model\n",
    "\n",
    "# Function to check and remove NaNs from dataset\n",
    "def dataChecker(arr):\n",
    "    idxRow = -1\n",
    "    for row in arr:\n",
    "        idxRow = idxRow + 1\n",
    "        for idx in range(len(row)):\n",
    "            if math.isnan(arr[idxRow,idx]) == True:\n",
    "                arr[idxRow, idx] = 0\n",
    "    return arr\n",
    "\n",
    "# Find max value in the dataset and its index\n",
    "def maxVal(arr):\n",
    "    idxRow = -1\n",
    "    maxVal = -100\n",
    "    indexes = np.empty(2)\n",
    "    for row in arr:\n",
    "        idxRow = idxRow + 1\n",
    "        for idx in range(len(row)):\n",
    "            if ((arr[idxRow,idx] > maxVal) and (idx != 0 and idx != 4 and idx != 5 and idx != 6 and idx != 7 and idx != 8)):\n",
    "                maxVal = arr[idxRow, idx]\n",
    "                indexes[0] = idxRow\n",
    "                indexes[1] = idx\n",
    "    return indexes, maxVal\n",
    "\n",
    "# Find max value in the dataset and its index\n",
    "def minVal(arr):\n",
    "    idxRow = -1\n",
    "    minVal = 100\n",
    "    indexes = np.empty(2)\n",
    "    for row in arr:\n",
    "        idxRow = idxRow + 1\n",
    "        for idx in range(len(row)):\n",
    "            if ((arr[idxRow,idx] < minVal) and (idx != 0 and idx != 4 and idx != 5 and idx != 6 and idx != 7 and idx != 8)):\n",
    "                minVal = arr[idxRow, idx]\n",
    "                indexes[0] = idxRow\n",
    "                indexes[1] = idx\n",
    "    return indexes, minVal\n",
    "\n",
    "# Scale all values in the array that are the waveform or waveform-dependent to a range\n",
    "def scaleVals(arrIn, arrOut, minAllowed, maxAllowed, minValue, maxValue):\n",
    "    idxRow = -1\n",
    "    for row in arrIn:\n",
    "        idxRow = idxRow + 1\n",
    "        for idx in range(len(row)):\n",
    "            if(idx != 0 and idx != 4 and idx != 5 and idx != 6 and idx != 7 and idx != 8):\n",
    "                scaled = (((maxAllowed - minAllowed) * (arrIn[idxRow,idx] - minValue)) / (maxValue - minValue)) + minAllowed\n",
    "                arrOut[idxRow, idx] = scaled\n",
    "            else:\n",
    "                arrOut[idxRow, idx] = arrIn[idxRow,idx]\n",
    "    return arrOut\n",
    "\n",
    "# Perform Recursive Feature Elimination to identify the 3 top features\n",
    "def RFE(arr):\n",
    "    #data = X, target = Y\n",
    "    X = arr[:,1:9]\n",
    "    Y = arr[:,0]\n",
    "\n",
    "    #Feature extraction\n",
    "    model = sklearn.linear_model.LogisticRegression() \n",
    "    rfeFeatures = sklearn.feature_selection.RFE(model, 3)\n",
    "    fit = rfeFeatures.fit(X,Y)\n",
    "    return fit.ranking_\n",
    "\n",
    "#  Number of waveforms for each neuron cell type \n",
    "valsFS = 1438775\n",
    "valsPT = 319484\n",
    "valsIT = 126460\n",
    "\n",
    "#  Number of rows in each array\n",
    "rows_FS = valsFS\n",
    "rows_PT = valsPT\n",
    "rows_IT = valsIT\n",
    "\n",
    "#  Separation value to split up training:testing sets (67:33)\n",
    "sep_FS = 2 * rows_FS // 3\n",
    "sep_PT = 2 * rows_PT // 3\n",
    "sep_IT = 2 * rows_IT // 3\n",
    "\n",
    "#  Create training sets\n",
    "col = 38\n",
    "trainArrSize = sep_FS\n",
    "train_set_FS = np.empty((trainArrSize,col))\n",
    "train_set_PT_attr = np.empty((trainArrSize,col))\n",
    "train_set_IT_attr = np.empty((trainArrSize,col))\n",
    "\n",
    "#  Fill the training sets with the 66% that is already existent (prior to oversampling)\n",
    "for indFS_init in range(sep_FS):\n",
    "    train_set_FS[indFS_init, :] = FS[indFS_init,:]\n",
    "\n",
    "for indPT_init in range(sep_PT):\n",
    "    train_set_PT_attr[indPT_init, :] = PT[indPT_init,:]\n",
    "\n",
    "for indIT_init in range(sep_IT):\n",
    "    train_set_IT_attr[indIT_init, :] = IT[indIT_init,:]\n",
    "\n",
    "#  Fill the test sets to completion\n",
    "test_set_FS = np.empty((0,col)) \n",
    "test_size_FS = valsFS - sep_FS\n",
    "test_set_PT = np.zeros((0,col)) \n",
    "test_size_PT = valsPT - sep_PT\n",
    "test_set_IT = np.zeros((0,col)) \n",
    "test_size_IT = valsIT - sep_IT\n",
    "\n",
    "test_set_FS = np.append(test_set_FS, FS[sep_FS:valsFS, :], axis = 0)\n",
    "test_set_PT = np.append(test_set_PT, PT[sep_PT:valsPT, :], axis = 0)\n",
    "test_set_IT = np.append(test_set_IT, IT[sep_IT:valsIT, :], axis = 0)\n",
    "\n",
    "#  Oversampling the minority with replacement\n",
    "\n",
    "#  Determine how much to add to PT/IT and size of pre-oversampling array\n",
    "numAdd_PT = sep_FS - sep_PT\n",
    "numAdd_IT = sep_FS - sep_IT\n",
    "trainPTArrSize = sep_PT\n",
    "trainITArrSize = sep_IT\n",
    "\n",
    "# Randomize attribute-wise (_attr) for all features but the waveform,\n",
    "#           which will be randomized as single unit\n",
    "for indPT_2 in range(trainPTArrSize,numAdd_PT+trainPTArrSize):\n",
    "    for attrPT in range(9):\n",
    "        rand = int(random() * (sep_PT+1))\n",
    "        train_set_PT_attr[indPT_2,attrPT] = train_set_PT_attr[rand, attrPT]\n",
    "    rand = int(random() * (sep_PT+1))\n",
    "    train_set_PT_attr[indPT_2, 9:] = train_set_PT_attr[rand, 9:]\n",
    "\n",
    "for indIT_2 in range(trainITArrSize,numAdd_IT+trainITArrSize):\n",
    "    for attrIT in range(9):\n",
    "        rand = int(random() * (sep_IT+1))\n",
    "        train_set_IT_attr[indIT_2,attrIT] = train_set_IT_attr[rand, attrIT]\n",
    "    rand = int(random() * (sep_IT+1))\n",
    "    train_set_IT_attr[indIT_2, 9:] = train_set_IT_attr[rand, 9:]\n",
    "\n",
    "#  Randomly combine individual training and testing sets into master training and testing sets\n",
    "train_set_attr = np.empty((trainArrSize * 3, col))\n",
    "countFS = 0\n",
    "countPT = 0\n",
    "countIT = 0\n",
    "indTrain= 0\n",
    "while indTrain < (trainArrSize * 3):\n",
    "    rand = int(random() * 3 + 1)\n",
    "    if rand == 1 and (countFS + 1 <= trainArrSize):\n",
    "        train_set_attr[indTrain,:] = train_set_FS[countFS,:]\n",
    "        countFS = countFS + 1\n",
    "        indTrain = indTrain + 1\n",
    "    elif rand == 2 and (countPT + 1 <= trainArrSize):\n",
    "        train_set_attr[indTrain,:] = train_set_PT_attr[countPT,:]\n",
    "        countPT = countPT + 1 \n",
    "        indTrain = indTrain + 1\n",
    "    elif rand == 3 and (countIT + 1 <= trainArrSize):\n",
    "        train_set_attr[indTrain,:] = train_set_IT_attr[countIT,:]\n",
    "        countIT = countIT + 1 \n",
    "        indTrain = indTrain + 1\n",
    "\n",
    "test_set = np.empty((test_size_FS + test_size_PT + test_size_IT, col))\n",
    "countFS = 0\n",
    "countPT = 0\n",
    "countIT = 0        \n",
    "indTest = 0\n",
    "while indTest < (test_size_FS + test_size_PT + test_size_IT):\n",
    "    rand = int(random() * 3 + 1)\n",
    "    if rand == 1 and (countFS + 1 <= test_size_FS):\n",
    "        test_set[indTest,:] = test_set_FS[countFS,:]\n",
    "        countFS = countFS + 1\n",
    "        indTest = indTest + 1\n",
    "    elif rand == 2 and (countPT + 1 <= test_size_PT):\n",
    "        test_set[indTest,:] = test_set_PT[countPT,:]\n",
    "        countPT = countPT + 1 \n",
    "        indTest = indTest + 1\n",
    "    elif rand == 3 and (countIT + 1 <= test_size_IT):\n",
    "        test_set[indTest,:] = test_set_IT[countIT,:]\n",
    "        countIT = countIT + 1 \n",
    "        indTest = indTest + 1\n",
    "\n",
    "# Remove NaNs in each array\n",
    "train_set_attr = dataChecker(train_set_attr)\n",
    "test_set = dataChecker(test_set)\n",
    "\n",
    "# Scaling inputs to 0-1\n",
    "    \n",
    "train_set_attr_scld = np.empty((2877549, 38))\n",
    "test_set_scld = np.empty((628241, 38))\n",
    "\n",
    "minValue = -0.00098502\n",
    "maxValue = 0.0011485\n",
    "\n",
    "train_set_attr_scld = scaleVals(train_set_attr, train_set_attr_scld, 0, 1, minValue, maxValue)\n",
    "test_set_scld = scaleVals(test_set, test_set_scld, 0, 1, minValue, maxValue)\n",
    "\n",
    "# Save files as a .csv \n",
    "np.savetxt('train_set_attr_scld.csv', train_set_attr_scld, delimiter = \",\")\n",
    "np.savetxt('test_set_scld.csv', test_set_scld, delimiter = \",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
